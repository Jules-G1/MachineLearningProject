---
title: "Introduction to Machine Learning - FIFA 2019 Case"
author: "A. Piguet - L. Gatinet - J. Guillot"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( warning = FALSE,
                       echo = TRUE,
                       message = FALSE,
                       highlight = TRUE)
```

```{r}
my_dir = getwd()
setwd(my_dir)
```

```{r}
library(readr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggExtra)
library(ranger)
library(caret)
library(ggcorrplot)
library(visNetwork)
library(rpart)
library(rpart.plot)
library(qdapRegex)
```

```{r}
data = read_csv("fifadata.csv")
```

# I-Data cleaning using dplyr:

To further use the 'Championship' variable in a relevant manner, we decided to remove championships for which some clubs are missing (namely championships FIFA 19 did not get the right to refer to) RAJOUTER SOURCE WIKI


#### Converting financial values as integers (Wage, Release Clause and Value) + Wage/Height
```{r}
data$Value = ifelse(substring(data$Value, 
                              nchar(data$Value)) == "M",
                              as.numeric(substring(data$Value,2,nchar(data$Value)-1)),
                              as.numeric(substring(data$Value,2,nchar(data$Value)-1)) / 1000)

data$Wage = as.numeric(substring(data$Wage,2,nchar(data$Wage)-1))

data$`Release Clause` = as.numeric(substring(data$`Release Clause`,2,nchar(data$`Release Clause`)-1))



data$Weight = as.numeric(substring(data$Weight, 1, 3)) * 0.45359237

data = data %>% 
  separate(Height, c('feet', 'inches'), "'", convert = TRUE) %>%
  mutate(Height = (12*feet + inches)*2.54) %>% 
  select(-inches, -feet)

data = data %>% rename(`Value (in M€)` = Value,
                        `Wage (in K€)` = Wage,
                        `Release Clause (in M€)` = `Release Clause`,
                        `Weight (in kg)` = Weight,
                        `Height (in cm)` = Height)
```


```{r}
num_club = as.numeric(ex_between(data$`Club Logo`, "light/", ".png"))
data$num_club = num_club

`%nin%` = Negate(`%in%`) # creating a 'not in' operator for convenience
club_table = read_csv('club.csv')
clean_club_table = club_table %>% filter(league_name %nin% c("South African Premier Division",
                                                        "Finnish Veikkausliiga",
                                                        "Greek Super League",
                                                        "Russian Premier League",
                                                        "Croatian Prva HNL",
                                                        "Czech Republic Gambrinus Liga",
                                                        "Ukrainian Premier League"))

group_club = clean_club_table %>% group_by(url) %>% summarize(league_name=first(league_name))
data = left_join(data,group_club,by=c('num_club'='url'))

df_league = data %>% 
  group_by(league_name) %>% 
  summarise(value=mean(`Value (in M€)`)) %>% 
  arrange(desc(value))
#Looking at the ranking of our leagues, we remark a gap in average player tranfer value between the 6th championship (the Portuguese one) and those following. For visualization purposes and as these first 6 championships are the most known ones, we decide to keep them and aggregate the others in under an "Other" category.
df_league_others = df_league %>% slice(7:nrow(df_league))
data[data$league_name %in% df_league_others$league_name, "league_name"] = "Others"
```

Removing the Photo, Flag, Club Logo, Loaned From columns // We removed scores of each players for each position on the field
Body Type removed because of high correlation with Height and Weight
```{r}
data = data %>% select(-Photo, 
                       -Flag, 
                       -(LS:RB), 
                       -Joined, 
                       -`Loaned From`, 
                       -X1, 
                       -Special, 
                       -`Real Face`, 
                       -`Body Type`, 
                       -`Jersey Number`,
                       -`Club Logo`,
                       -num_club)
```

Create categories for field positions

```{r}
data$Field_Position = rep(0,nrow(data))
for (i in 1:nrow(data)) {
  if (data$Position[i] %in% c("LW", "LF", "RW", "RF", 'CF', 'LS', 'RS', 'ST')) data$Field_Position[i] = "Attack"
  if (data$Position[i] %in% c('CDM', 'LDM', 'RDM', 'CM', 'LCM', 'RCM', 'CAM', 'LAM', 'RAM', 'LM', 'RM')) data$Field_Position[i] = "Mitfielder"
  if (data$Position[i] %in% c('CB', 'LCB', 'RCB', 'LB', 'RB', 'LWB', 'RWB')) data$Field_Position[i] = "Defenser"
  if (data$Position[i] %in% c("GK")) data$Field_Position[i] = "Goalkeeper"
}
```

#### Splitting the Work Rate column into its 2 components (Offensive / Defensive)
```{r}
data = data %>% separate(`Work Rate`, c('Offensive Work Rate', 'Defensive Work Rate'), "/ ")

data$`Offensive Work Rate` = factor(data$`Offensive Work Rate`, 
                                    levels=c("Low", "Medium", 'High'))
data$`Defensive Work Rate` = factor(data$`Defensive Work Rate`, 
                                    levels=c("Low", "Medium", 'High'))
data$Field_Position = factor(data$Field_Position, 
                             levels=c("Goalkeeper", "Defenser", 'Mitfielder', "Attack"))
data$`Preferred Foot` = factor(data$`Preferred Foot`, 
                               levels=c("Left", "Right"))
data$`International Reputation` = factor(data$`International Reputation`, 
                                         levels=1:5)
data$`Weak Foot` = factor(data$`Weak Foot`, 
                          levels=1:5)
data$`Skill Moves` = factor(data$`Skill Moves`, 
                            levels=1:5)
data$Club <- as.factor(data$Club)
```

#### Creating a column that captures the remaining duration of the player's contract

```{r}
text = NULL; first = NULL; last = NULL
for (i in 1:nrow(data)) {
  text[i] = data$`Contract Valid Until`[i] 
  first[i] = nchar(text[i])-4
  last[i] = nchar(text[i])
  data$`Contract Valid Until`[i] = substring(text[i],first[i],last[i])
}
data$`Remaining Contract Duration` = as.numeric(data$`Contract Valid Until`) - 2018
data <- data %>% select(-`Contract Valid Until`, -Position)
```


```{r}
df = data[which(apply(is.na(data),1,any)),]
head(df)
```

All remaining NA's are players who have been loaned to another team (1264 players) 

nad 300 other players for whom FIFA did not gather the information or was denied the access

```{r}
data = na.omit(data)
```

# II-Data Exploration using dplyr & ggplot

```{r}
ggplot(data) + 
  aes(x=`Value (in M€)`) + 
  geom_histogram(bins=30) + 
  scale_x_log10() +
  theme_bw()
```

#### Plot of the Value vs Remaining Contract Years
```{r}
ggplot(data) +
  aes(x=`Wage (in K€)`, y=`Value (in M€)`, colour=Field_Position) +
  geom_point(stroke=F) +
  geom_smooth(method='lm') +
  scale_x_continuous(limits=c(0,300)) +
  scale_color_discrete("Field Position") +
  theme_bw() +
  theme(axis.title=element_text(face="bold"))
```

As expected, the players' market value is positively correlated to the players' wages. Looking at the different field positions we see that, for a given market value, Defensers get a better wage than Attackers. This reflects the law of offer and supply: there are more Attackers valued at 30 M€ than Defensers so the latter get better paid.

#### Plot
```{r}
ggplot(data) +
  aes(x=`Offensive Work Rate`, y=`Value (in M€)`, fill=`Offensive Work Rate`) +
  geom_boxplot(show.legend = F) +
  scale_y_log10() +
  theme_bw()
```


#### Plot
```{r}
ggplot(data) +
  aes(group=Overall, y=`Value (in M€)`, fill=Overall) +
  geom_boxplot(colour="Black", size=0.3) +
  scale_fill_gradient2(low="Red", mid="Red", high="Blue", midpoint=75) +
  scale_x_continuous(limits=c(0.1,0.4)) +
  facet_grid(Field_Position~.) +
  theme(legend.position = "bottom")
```


#### Plot
```{r}
dfgraph <- data %>% 
  group_by(Age) %>% 
  summarize(Generation=n()) %>% 
  mutate(GenPerc = Generation/sum(Generation)) %>% 
  right_join(data, by="Age")

ggplot(dfgraph) +
  aes(x=Age, y=`Value (in M€)`, fill=GenPerc) +
  geom_boxplot(aes(group=Age), alpha=0.5, size=0.3) +
  scale_x_continuous(limits=c(15,42)) +
  scale_y_log10() +
  scale_fill_gradient("% of Tot. Population", low="yellow", high="red") +
  theme_bw()+
  theme(legend.position = "bottom",
        legend.title = element_text(face="bold"))
```

#### Plot
```{r}
plot_central = ggplot(data) + 
                  aes(x=Age, y=Overall, colour=log(`Value (in M€)`)) +
                  geom_point(stroke=F, alpha=0.7) +
                  geom_hline(yintercept = median(data$Overall), lty=2, alpha=0.5) +
                  geom_vline(xintercept = median(data$Age), lty=2, alpha=0.5) +
                  geom_smooth(method="loess", size=0.3, se=F, colour="black", show.legend = T) +
                  theme_bw() +
                  scale_color_continuous("Transfer Value", low="white", high="blue", labels=NULL) +
                  scale_x_continuous(limits=c(16, 40)) +
                  
                  theme(legend.position="bottom", legend.title = element_text(size=10),
                        axis.title.x = element_text(face="bold"),
                        axis.title.y = element_text(face="bold"))
ggMarginal(plot_central, type="boxplot", col="darkblue")
```


***

# III-Regression models : Predict Value with inputs

Correlogram of the dataset:

```{r}
datacor <- data %>% select(-Name,
                           -Nationality,
                           -Club,
                           -league_name,
                           -Field_Position)

datacor$`Preferred Foot` <- as.numeric(datacor$`Preferred Foot` == "Right")
datacor$`International Reputation` <- as.numeric(datacor$`International Reputation`)
datacor$`Weak Foot` <- as.numeric(datacor$`Weak Foot`)
datacor$`Skill Moves` <- as.numeric(datacor$`Skill Moves`)
datacor$`Offensive Work Rate` <- as.numeric(datacor$`Offensive Work Rate`)
datacor$`Defensive Work Rate` <- as.numeric(datacor$`Defensive Work Rate`)
str(datacor)
corr <- cor(datacor)


ggcorrplot(corr, method = "square", tl.cex=5, tl.srt=60)
```

#### 1. Linear regression (full)

```{r}
datareg <- data %>% select(-ID, -Name, -Nationality, -Club)
#EXPLAIN MORE WHY WE DELETE THESE VARIABLES (corr, dummies, pas besoin koi)
#We do a 10-folds cross validation
#Warning message on Position
```

```{r}
library(caret)
folds <- createFolds(1:nrow(datareg),k=20)
```

```{r, warning=F}
pred_lm =  rep(NA,nrow(datareg))

for (i in 1:20){
  train <- datareg %>% slice(-folds[[i]])
  test <- datareg %>% slice(folds[[i]])
  linear_model <- lm(`Value (in M€)`~., data=train)
  pred_lm[folds[[i]]]=predict(linear_model,newdata=test)
}

mean((pred_lm-datareg$`Value (in M€)`)^2)
```


RIDGE
```{r}
library(glmnet)

X <- model.matrix(`Value (in M€)`~.,data=datareg)[,-1]

#Find the best lambda for ridge
my_ridge = cv.glmnet(X,datareg$`Value (in M€)`,alpha = 0,lambda=exp(seq(-8,4,length=100)))
plot(my_ridge)
best_lambda_ridge = my_ridge$lambda.min

pred_ridge_linear =  rep(NA,nrow(datareg))
for (i in 1:20){
  train.X = X[-folds[[i]],]
  train.Y = datareg$`Value (in M€)`[-folds[[i]]]
  test.X = X[folds[[i]],]
  ridge_linear = glmnet(train.X,train.Y,alpha = 0)
  pred_ridge_linear[folds[[i]]]=as.vector(predict(ridge_linear,newx=test.X,s=best_lambda_ridge))

}
```

LASSO
```{r}

#Find the best lambda for lasso
my_lasso = cv.glmnet(X,datareg$`Value (in M€)`,alpha = 1,lambda=exp(seq(-40,4,length=200)))
plot(my_lasso)
best_lambda_lasso = my_lasso$lambda.min


pred_lasso_linear =  rep(NA,nrow(datareg))
for (i in 1:20){
  train.X = X[-folds[[i]],]
  train.Y = datareg$`Value (in M€)`[-folds[[i]]]
  test.X = X[folds[[i]],]
  lasso_linear = glmnet(train.X,train.Y,alpha = 1)
  pred_lasso_linear[folds[[i]]]=as.vector(predict(lasso_linear,newx=test.X,s=best_lambda_lasso))

}

```
COMPARISON

```{r}
prev_df=data.frame(pred_lm,pred_ridge_linear,pred_lasso_linear,observed=datareg$`Value (in M€)`)

prev_df%>%summarize_all(~mean((.-observed)^2)) 
```

```{r}
#First we compute residuals for all models
df_residuals=data.frame(index = seq(1,nrow(prev_df)),
                        res_lm = prev_df$observed - prev_df$pred_lm,
                        res_ridge= prev_df$observed - prev_df$pred_ridge_linear,
                        res_lasso= prev_df$observed - prev_df$pred_lasso_linear)

#We take the index of top 10 highest residuals for each model
high_res_lm=head(order(abs(df_residuals$res_lm),decreasing = T),10)
high_res_ridge=head(order(abs(df_residuals$res_ridge),decreasing = T),10)
high_res_lasso=head(order(abs(df_residuals$res_lasso),decreasing = T),10)

#We map which players are the outliers : they are almost same players for each model
residuals_players= data.frame(data[high_res_lm,'Name'],
                              data[high_res_ridge,'Name'],
                              data[high_res_lasso,'Name'])
colnames(residuals_players)=c('lm','ridge','lasso')
residuals_players

df_residuals2=df_residuals %>% gather(key = model, value = residuals, - index)

#We visualise residuals plot to see that we have large outliers  but not too many
ggplot(df_residuals2) + aes(x = index, y = residuals, color = model) + geom_point() + facet_grid(~model)

#We could remove them from the sample to see that the MSE is better
MSE = prev_df%>%slice(-(1:2000))%>%summarize_all(~mean((.-observed)^2))
MSE
```
#But with deeper analysis we understand that the MAPE remains quite high with this change because we have lower values but still a high comparative error


```{r, warning=F, message=F}
MAPE = prev_df%>%slice(-(1:2000))%>%summarize_all(~mean(abs((observed-.)/(observed))))
MAPE
```
We conclude that we won't remove these outliers from the sample


**Non parametric model : KNN**

First, find the best k with caret
```{r}
library(class)

X <- model.matrix(`Value (in M€)`~.,data=datareg)[,-1]
dataknn = data.frame(Y=datareg$`Value (in M€)`,X)

grid.k = data.frame(k=seq(1,5,by=1)) #I took a low number to reduce comuting time
ctrl = trainControl(method="cv",number=5) #I took a low number to reduce comuting time
select.k = train(Y~.,data =  dataknn, method="knn",trControl=ctrl,tuneGrid=grid.k)
best.k=select.k$bestTune

```

Then, we compute knn with the best k
```{r}

pred_knn=rep(NA,nrow(dataknn))
for (i in 1:20) {
pred_knn[folds[[i]]]=as.numeric(as.vector(knn(train = dataknn[-folds[[i]],2:69] , test = dataknn[folds[[i]],2:69] ,cl = dataknn[-folds[[i]],1], k=best.k)))
}

```
Let's analyse our results : 

```{r}

prev_df_knn = data.frame(pred_knn, Y = dataknn$Y)

MSE_knn = prev_df_knn %>% summarize(MSE_knn=mean((pred_knn-Y)^2))
MSE_knn
#Better results

```


**Non parametric model : Kernel**

As we need only 1 variable as input in this model, we have decided to do a pca analysis and keep only the first PC.

PCA methodology

```{r}
library(KernSmooth)

datakernel = data.frame(Y = data$`Value (in M€)`, X)
my.pca = princomp(datakernel[,-1])
summary(my.pca)#We take only the first PC to do kernel regression (it represents almost 90% of the total variance so we are satisfied)
loadings=as.matrix(my.pca$loadings[,1]) #Just to visualise the coefficient of each variable : it is difficult to interpret
datakernel_pca = data.frame(X = my.pca$scores[,1], Y = datakernel$Y) #We take only scores for each player of the first PC
plot(datakernel_pca[1:500,]) #For the first 500 players, it seems that there is a negative relation between pca scores and market values.
plot(datakernel_pca) #Nevertheless, for the whole data it seems that there is no relation between scores and market values but let's test it :
```

Find the best bandwidth and results
```{r}
pred_kernel = rep(NA,nrow(datakernel_pca))
MSE=rep(NA,100)

for (j in 1:200) {
    for (i in 1:20){
    kernel =  ksmooth(datakernel_pca[-folds[[i]],1],
                      datakernel_pca[-folds[[i]],2],
                      bandwidth = j/10, #we want to test bandwidths from 0.1 to 20
                      x.points = datakernel[folds[[i]],1])
    
    pred_kernel[folds[[i]]] = kernel$y 
    }
MSE[j] = mean((pred_kernel-datakernel_pca$Y)^2,na.rm=T) 
}

min(MSE) 
#The lowest MSE is incredibly high : the kernel model with PCA is not relevant for our business problem

```


#### 2. Trees and Random forests

```{r}
pred.tree= as.double(rep(NA,nrow(datareg)))

cp_opt = rep (NA,20)
for (i in 1:20) {
train <- datareg %>% slice(-folds[[i]])
test <- datareg %>% slice(folds[[i]])
tree <- rpart(`Value (in M€)`~., data=train)
cp_opt [i] <- tree$cptable %>% 
          as.data.frame() %>% 
          filter(xerror==min(xerror)) %>% 
          select(CP) %>% as.numeric()
besttree <- prune(tree,cp=cp_opt[i])
pred.tree[folds[[i]]] <-predict(besttree,newdata = test)
}

prev_df <- (prev_df %>% mutate(pred.tree))[,c(1,2,3,5,4)]

prev_df %>% summarize_all(~mean((.-observed)^2))
#Tree has better MSE

print(cp_opt)
#We keep the same cp_opt which could mean that there is no much variation between tree used for each fold

rpart.plot::rpart.plot(besttree)
visTree(besttree)
```

```{r}
X<-as.data.frame(datareg[,-4])
Y<-as.vector(datareg[,4])
datareg2<-data.frame(X,Y)

summary(datareg2)
class(datareg)


class(datareg2)

RF_small <- ranger(Y ~., data=datareg2, num.trees =500, mtry=16)
#RF_big <- ranger:: ranger(Value..in.M..~., data=datareg2, num.trees =2500, mtry=50)
print(RF_small)
#print(RF_big)
#comparo<-data.frame(RF_small$predictions,RF_big$predictions, Y)
#colMeans(data.frame((RF_small$predictions-Y)^2,(RF_big$predictions-Y)^2))
plot(RF_small)
#RF$prediction.error
#plot(RF)
#ranger:: print.ranger.prediction(RF)
#ranger:: importance.ranger(RF)
#predict()

grille.mtry <- data.frame(mtry=seq(1,50,by=5))
ctrl <- trainControl(method="oob")
sel.mtry <- train(Y~.,data=datareg2,method="rf",trControl=ctrl,
                  tuneGrid=grille.mtry)
sel.mtry$bestTune
```
